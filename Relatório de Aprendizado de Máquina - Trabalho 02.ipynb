{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regressão Logística com Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta parte do trabalho, será implementada a **_Regressão Logística Regularizada_**\n",
    "para prever se os microchips de uma usina de fabricação passam na garantia\n",
    "de qualidade (QA). Durante a QA, cada microchip passa por vários testes para\n",
    "garantir se está funcionando corretamente. Dessa forma, a Gestão de Produto da \n",
    "fábrica terá o resultados de teste para alguns microchips em dois testes diferentes. \n",
    "\n",
    "A partir desses dois testes, será determinado se os microchips deveriam ser \n",
    "aceitos ou rejeitados. Para auxiliar a tomar a decisão, há um conjunto de dados\n",
    "com resultados de testes anteriores sobre microchips, a partir do qual é possível construir\n",
    "um modelo de Regressão Logística.\n",
    "\n",
    "O arquivo **$ex2data2.txt$** contém os dados a serem usados nessa parte do trabalho. A primeira \n",
    "coluna corresponde aos resultados do primeiro teste, enquanto que a segunda coluna corresponde\n",
    "aos resultados do segundo teste. A terceira coluna contém os valores da classe (y = 0 significa \n",
    "rejeitado no teste, e y = 1 significa aceito no teste).\n",
    "\n",
    "## 1.1 Visualização dos Dados\n",
    "\n",
    "Para a maioria dos conjuntos de dados do mundo real, não é possível criar um gráfico para \n",
    "visualizar seus pontos. Mas, para o conjunto de dados fornecido, isso é possível. Implemente \n",
    "um script em _Python_ que produza um gráfico de dispersão (scatter plot) dos dados fornecidos. \n",
    "Após finalizado, seu script deve produzir um resultado similar ao apresentado na Figura abaixo.\n",
    "\n",
    "![ScatterPlot](scatter_plot2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from numpy import loadtxt, zeros, ones, array, linspace, logspace\n",
    "from pylab import scatter, show, legend, xlabel, ylabel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QATest1</th>\n",
       "      <th>QATest2</th>\n",
       "      <th>QAcceptance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051267</td>\n",
       "      <td>0.69956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.092742</td>\n",
       "      <td>0.68494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213710</td>\n",
       "      <td>0.69225</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.50219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.513250</td>\n",
       "      <td>0.46564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    QATest1  QATest2  QAcceptance\n",
       "0  0.051267  0.69956            1\n",
       "1 -0.092742  0.68494            1\n",
       "2 -0.213710  0.69225            1\n",
       "3 -0.375000  0.50219            1\n",
       "4 -0.513250  0.46564            1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construindo um dataset com base num Dataframe, já identificando colunas e exibindo seus primeiros 20 registros.\n",
    "df = pd.read_csv('am-T2-dados/ex2data2.txt', names=['QATest1', 'QATest2', 'QAcceptance'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvr/kdS0EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116d862b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117058ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualização da Distribuição dos Dados conforme Histograma\n",
    "\n",
    "df.hist()\n",
    "plt.pyplot.show()\n",
    "\n",
    "df.QATest1.hist(), df.QATest2.hist(), df.QAcceptance.hist()\n",
    "plt.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QATest1</th>\n",
       "      <th>QATest2</th>\n",
       "      <th>QAcceptance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.054779</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.491525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496654</td>\n",
       "      <td>0.519743</td>\n",
       "      <td>0.502060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.830070</td>\n",
       "      <td>-0.769740</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.372120</td>\n",
       "      <td>-0.254385</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.213455</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.478970</td>\n",
       "      <td>0.646562</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.070900</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          QATest1     QATest2  QAcceptance\n",
       "count  118.000000  118.000000   118.000000\n",
       "mean     0.054779    0.183102     0.491525\n",
       "std      0.496654    0.519743     0.502060\n",
       "min     -0.830070   -0.769740     0.000000\n",
       "25%     -0.372120   -0.254385     0.000000\n",
       "50%     -0.006336    0.213455     0.000000\n",
       "75%      0.478970    0.646562     1.000000\n",
       "max      1.070900    1.108900     1.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118.00\n",
       "mean       0.05\n",
       "std        0.50\n",
       "min       -0.83\n",
       "25%       -0.37\n",
       "50%       -0.01\n",
       "75%        0.48\n",
       "max        1.07\n",
       "Name: QATest1, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.QATest1.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118.00\n",
       "mean       0.18\n",
       "std        0.52\n",
       "min       -0.77\n",
       "25%       -0.25\n",
       "50%        0.21\n",
       "75%        0.65\n",
       "max        1.11\n",
       "Name: QATest2, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.QATest2.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118.00\n",
       "mean       0.49\n",
       "std        0.50\n",
       "min        0.00\n",
       "25%        0.00\n",
       "50%        0.00\n",
       "75%        1.00\n",
       "max        1.00\n",
       "Name: QAcceptance, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.QAcceptance.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAF3CAYAAADOyc2FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11692e860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfico de correlação entre População e Lucro\n",
    "positive = df[df['QAcceptance'].isin([1])]  \n",
    "negative = df[df['QAcceptance'].isin([0])]\n",
    "\n",
    "figura, grafico = plt.pyplot.subplots(figsize=(10,6))  \n",
    "grafico.scatter(positive['QATest1'], positive['QATest2'], c='xkcd:black', marker='+', label='Accepted')  \n",
    "grafico.scatter(negative['QATest1'], negative['QATest2'], c='xkcd:yellow', marker='o', label='Rejected')  \n",
    "grafico.legend()  \n",
    "grafico.set_xlabel('Microchip Test1')  \n",
    "grafico.set_ylabel('Microchip Test2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Mapeamento de características (_feature mapping_)\n",
    "\n",
    "Uma maneira de tornar os dados mais apropriados para a classificação é criar\n",
    "mais características a partir das já existentes. Para isso, você deve criar uma\n",
    "função **`mapFeature`**. Essa função deve ser implementada em um arquivo de\n",
    "nome **_`mapFeature.py`_**, que irá mapear as características para todos os termos\n",
    "polinomiais de x1 e x2, até a sexta potência. Como resultado desse mapeamento, nosso \n",
    "vetor de duas características (os escores nos dois testes de QA) será transformado \n",
    "em um **vetor de _28 dimensões_**.\n",
    "\n",
    "Um classificador que usa regressão logística treinado nesse vetor de características \n",
    "de maior dimensão terá uma fronteira de decisão mais complexa e parecerá não-linear \n",
    "quando desenhado em um gráfico bidimensional. \n",
    "\n",
    "Embora o mapeamento de características nos permita construir um classificador mais expressivo, \n",
    "também é mais suscetível a sobreajuste (overfitting). Desse modo, será implementada a \n",
    "**_Regressão Logística Regularizada_** sobre os dados fornecidos e também verá como a regularização pode \n",
    "ajudar a combater o problema do sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dF = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapFeature(X) :\n",
    "    '''\n",
    "    Função de mapeamento das características\n",
    "    '''\n",
    "    potencia = 6\n",
    "    Xf = X\n",
    "    \n",
    "    X1 = Xf['QATest1']  \n",
    "    X2 = Xf['QATest2']\n",
    "\n",
    "    Xf.insert(3, 'Ones', 1)\n",
    "\n",
    "    for i in range(potencia+1):  \n",
    "        for j in range(i):\n",
    "            Xf['F' + str(i) + str(j)] = np.power(X1, i-j) * np.power(X2, j)\n",
    "\n",
    "    #Xf.drop('QATest1', axis=1, inplace=True)  \n",
    "    #Xf.drop('QATest2', axis=1, inplace=True)\n",
    "\n",
    "    return Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QATest1</th>\n",
       "      <th>QATest2</th>\n",
       "      <th>QAcceptance</th>\n",
       "      <th>Ones</th>\n",
       "      <th>F10</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F30</th>\n",
       "      <th>F31</th>\n",
       "      <th>F32</th>\n",
       "      <th>...</th>\n",
       "      <th>F51</th>\n",
       "      <th>F52</th>\n",
       "      <th>F53</th>\n",
       "      <th>F54</th>\n",
       "      <th>F60</th>\n",
       "      <th>F61</th>\n",
       "      <th>F62</th>\n",
       "      <th>F63</th>\n",
       "      <th>F64</th>\n",
       "      <th>F65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1.180000e+02</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.054779</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054779</td>\n",
       "      <td>0.247575</td>\n",
       "      <td>-0.025472</td>\n",
       "      <td>5.983333e-02</td>\n",
       "      <td>0.030682</td>\n",
       "      <td>0.015483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>9.432094e-03</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>4.089084e-03</td>\n",
       "      <td>7.837118e-02</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>1.893340e-02</td>\n",
       "      <td>-0.001705</td>\n",
       "      <td>2.259170e-02</td>\n",
       "      <td>-0.006302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496654</td>\n",
       "      <td>0.519743</td>\n",
       "      <td>0.502060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496654</td>\n",
       "      <td>0.248532</td>\n",
       "      <td>0.224075</td>\n",
       "      <td>2.746459e-01</td>\n",
       "      <td>0.134706</td>\n",
       "      <td>0.150143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072274</td>\n",
       "      <td>5.455787e-02</td>\n",
       "      <td>0.058513</td>\n",
       "      <td>9.993907e-02</td>\n",
       "      <td>1.938621e-01</td>\n",
       "      <td>0.058271</td>\n",
       "      <td>3.430092e-02</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>4.346935e-02</td>\n",
       "      <td>0.090621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.830070</td>\n",
       "      <td>-0.769740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.830070</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.484096</td>\n",
       "      <td>-5.719317e-01</td>\n",
       "      <td>-0.358121</td>\n",
       "      <td>-0.483743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246068</td>\n",
       "      <td>-1.592528e-01</td>\n",
       "      <td>-0.142660</td>\n",
       "      <td>-4.830370e-01</td>\n",
       "      <td>6.472253e-14</td>\n",
       "      <td>-0.203971</td>\n",
       "      <td>2.577297e-10</td>\n",
       "      <td>-0.113448</td>\n",
       "      <td>2.418097e-10</td>\n",
       "      <td>-0.482684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.372120</td>\n",
       "      <td>-0.254385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.372120</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>-0.178209</td>\n",
       "      <td>-5.155632e-02</td>\n",
       "      <td>-0.023672</td>\n",
       "      <td>-0.042980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>-3.659760e-03</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>-7.449462e-03</td>\n",
       "      <td>8.086369e-05</td>\n",
       "      <td>-0.006381</td>\n",
       "      <td>1.258285e-04</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>3.528590e-04</td>\n",
       "      <td>-0.016662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.213455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>0.165397</td>\n",
       "      <td>-0.016521</td>\n",
       "      <td>-2.544062e-07</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-1.473547e-07</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-8.972096e-09</td>\n",
       "      <td>4.527344e-03</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.387050e-03</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>3.921378e-03</td>\n",
       "      <td>-0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.478970</td>\n",
       "      <td>0.646562</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478970</td>\n",
       "      <td>0.389925</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>1.099616e-01</td>\n",
       "      <td>0.086392</td>\n",
       "      <td>0.079510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019183</td>\n",
       "      <td>1.370560e-02</td>\n",
       "      <td>0.021148</td>\n",
       "      <td>2.751341e-02</td>\n",
       "      <td>5.932959e-02</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>2.090875e-02</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>2.103622e-02</td>\n",
       "      <td>0.001289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.070900</td>\n",
       "      <td>1.108900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.070900</td>\n",
       "      <td>1.146827</td>\n",
       "      <td>0.568307</td>\n",
       "      <td>1.228137e+00</td>\n",
       "      <td>0.449251</td>\n",
       "      <td>0.505577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304409</td>\n",
       "      <td>2.451845e-01</td>\n",
       "      <td>0.287323</td>\n",
       "      <td>4.012965e-01</td>\n",
       "      <td>1.508320e+00</td>\n",
       "      <td>0.250577</td>\n",
       "      <td>2.018260e-01</td>\n",
       "      <td>0.183548</td>\n",
       "      <td>2.556084e-01</td>\n",
       "      <td>0.436209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          QATest1     QATest2  QAcceptance   Ones         F10         F20  \\\n",
       "count  118.000000  118.000000   118.000000  118.0  118.000000  118.000000   \n",
       "mean     0.054779    0.183102     0.491525    1.0    0.054779    0.247575   \n",
       "std      0.496654    0.519743     0.502060    0.0    0.496654    0.248532   \n",
       "min     -0.830070   -0.769740     0.000000    1.0   -0.830070    0.000040   \n",
       "25%     -0.372120   -0.254385     0.000000    1.0   -0.372120    0.043243   \n",
       "50%     -0.006336    0.213455     0.000000    1.0   -0.006336    0.165397   \n",
       "75%      0.478970    0.646562     1.000000    1.0    0.478970    0.389925   \n",
       "max      1.070900    1.108900     1.000000    1.0    1.070900    1.146827   \n",
       "\n",
       "              F21           F30         F31         F32     ...      \\\n",
       "count  118.000000  1.180000e+02  118.000000  118.000000     ...       \n",
       "mean    -0.025472  5.983333e-02    0.030682    0.015483     ...       \n",
       "std      0.224075  2.746459e-01    0.134706    0.150143     ...       \n",
       "min     -0.484096 -5.719317e-01   -0.358121   -0.483743     ...       \n",
       "25%     -0.178209 -5.155632e-02   -0.023672   -0.042980     ...       \n",
       "50%     -0.016521 -2.544062e-07    0.006603   -0.000039     ...       \n",
       "75%      0.100795  1.099616e-01    0.086392    0.079510     ...       \n",
       "max      0.568307  1.228137e+00    0.449251    0.505577     ...       \n",
       "\n",
       "              F51           F52         F53           F54           F60  \\\n",
       "count  118.000000  1.180000e+02  118.000000  1.180000e+02  1.180000e+02   \n",
       "mean     0.011812  9.432094e-03    0.018278  4.089084e-03  7.837118e-02   \n",
       "std      0.072274  5.455787e-02    0.058513  9.993907e-02  1.938621e-01   \n",
       "min     -0.246068 -1.592528e-01   -0.142660 -4.830370e-01  6.472253e-14   \n",
       "25%     -0.001926 -3.659760e-03   -0.001400 -7.449462e-03  8.086369e-05   \n",
       "50%      0.000205 -1.473547e-07    0.001026 -8.972096e-09  4.527344e-03   \n",
       "75%      0.019183  1.370560e-02    0.021148  2.751341e-02  5.932959e-02   \n",
       "max      0.304409  2.451845e-01    0.287323  4.012965e-01  1.508320e+00   \n",
       "\n",
       "              F61           F62         F63           F64         F65  \n",
       "count  118.000000  1.180000e+02  118.000000  1.180000e+02  118.000000  \n",
       "mean    -0.000703  1.893340e-02   -0.001705  2.259170e-02   -0.006302  \n",
       "std      0.058271  3.430092e-02    0.037443  4.346935e-02    0.090621  \n",
       "min     -0.203971  2.577297e-10   -0.113448  2.418097e-10   -0.482684  \n",
       "25%     -0.006381  1.258285e-04   -0.005749  3.528590e-04   -0.016662  \n",
       "50%     -0.000004  3.387050e-03   -0.000005  3.921378e-03   -0.000020  \n",
       "75%      0.002104  2.090875e-02    0.001024  2.103622e-02    0.001289  \n",
       "max      0.250577  2.018260e-01    0.183548  2.556084e-01    0.436209  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfeature = mapFeature(dF)\n",
    "dfeature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Função de custo e gradiente\n",
    "\n",
    "Agora, você deverá implementar o código para calcular **_a função de custo e\n",
    "o gradiente para a regressão logística regularizada_**. Crie um arquivo de nome\n",
    "costFunctionReg.py que contém uma função de nome costFunctionReg.py\n",
    "e que computa o custo e o gradiente. Lembre-se de que a função de custo\n",
    "regularizada na regressão logística é dada por:\n",
    "    \n",
    "$$J_{regularizado} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(x^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- x^{(i)}\\right) \\large{)} }_\\text{Função de Custo} + \\underbrace{\\frac{\\lambda}{2m} \\sum\\limits_{j = 1}^{n}\\ {\\theta}_{j}^{2} }_\\text{Fator Regularização} $$\n",
    "    \n",
    "Depois de concluir a implementação da função **costFunctionReg**, você deve\n",
    "testar a corretude dela usando o valor inicial de ${\\theta}$ (inicializado todo com zeros).\n",
    "Você deve ver que o custo é de cerca de `0.693`.\n",
    "Porém, usando a função **costFunctionReg**, você agora deve computar os valores ótimos para ${\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    A função sigmoid\n",
    "    '''\n",
    "    g = np.array([x]).flatten()\n",
    "    s = 1 / (1 + np.exp(-g))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, alpha): \n",
    "    '''\n",
    "    A versão do GD que acabamos de estudar é denominada *Batch gradienteient Descent*.\n",
    "    Em cada iteração do algoritmo, todo o conjunto de treinamento é utilizado.\n",
    "\n",
    "        X = características (features)\n",
    "        y = alvo (target)\n",
    "        alpha = taxa de aprendizado\n",
    "        regularizacao = Taxa de regularizacaoularização\n",
    "\n",
    "    '''\n",
    "    \n",
    "    m = y.size\n",
    "    H = sigmoid(X.dot(theta).T)\n",
    "    \n",
    "    J = (1/m) * np.sum( np.multiply(-y, np.log(H)) - np.multiply((1 - y), np.log(1 - H)) )\n",
    "    regularizacao = (alpha/2*m) * theta**2\n",
    "    \n",
    "    J_regular = J + regularizacao\n",
    "\n",
    "    parameters = theta.shape[1]\n",
    "    gradiente = np.zeros(parameters)\n",
    "    error = sigmoid(X * theta.T) - y\n",
    "\n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, X[:,i])\n",
    "        if (i == 0):\n",
    "            gradiente[i] = np.sum(term)/m\n",
    "        else:\n",
    "            gradiente[i] = np.sum(term)/m + ((alpha/m) * theta[:,i])\n",
    "\n",
    "    return J, gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Testando a Função de Custo e o Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set X and y (remember from above that we moved the label to column 0)\n",
    "#df.QATest1\n",
    "#df.QATest2\n",
    "\n",
    "cols = df.shape[1]  \n",
    "X2 = df.iloc[:,2]  \n",
    "y2 = df.iloc[:,:2]\n",
    "\n",
    "# convert to numpy arrays and initalize the parameter array theta\n",
    "X2 = np.array(X2.values)  \n",
    "y2 = np.array(y2.values)  \n",
    "theta2 = np.zeros(11)\n",
    "\n",
    "learningRate = 0.01\n",
    "\n",
    "costFunctionReg(theta2, X2, y2, learningRate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Esboço da fronteira de decisão\n",
    "\n",
    "Nessa parte, você deve esboçar (plotar) a fronteira de decisão que foi aprendida\n",
    "para separar os exemplos positivos dos negativos. Crie uma arquivo de nome\n",
    "**``plotDecisionBoundary.py``**, para criar esse gráfico que traça o limite da decisão\n",
    "não-linear. Seu gráfico deve ser semelhante ao apresentado na **_Figura_** abaixo.\n",
    "\n",
    "![ScatterBoundaryPlot](scatter_boundary.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mapFeature() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0de86676eaed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmapFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mapFeature() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Plot Boundary\n",
    "u = linspace(-1, 1.5, 50)\n",
    "v = linspace(-1, 1.5, 50)\n",
    "z = np.zeros(shape=(len(u), len(v)))\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i, j] = (mapFeature(array(u[i]), array(v[j])).dot(array(theta)))\n",
    "\n",
    "z = z.T\n",
    "contour(u, v, z)\n",
    "title('lambda = %f' % l)\n",
    "xlabel('Microchip Test 1')\n",
    "ylabel('Microchip Test 2')\n",
    "legend(['y = 1', 'y = 0', 'Decision boundary'])\n",
    "show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def plotDecisionBoundary(X, y, theta, alpha, iteracao):\n",
    "#    '''\n",
    "#    A versão do GD que acabamos de estudar é denominada *Batch Gradient Descent*.\n",
    "#    Em cada iteração do algoritmo, todo o conjunto de treinamento é utilizado.\n",
    "#\n",
    "#        X = características (features)\n",
    "#        y = alvo (target)\n",
    "#    '''\n",
    "#    \n",
    "#    u = linspace(-1, 1.5, 50)\n",
    "#    v = linspace(-1, 1.5, 50)\n",
    "#    z = zeros(shape=(len(u), len(v)))\n",
    "#    for i in range(len(u)):\n",
    "#        for j in range(len(v)):\n",
    "#            z[i, j] = (map_feature(array(u[i]), array(v[j])).dot(array(theta)))\n",
    "#\n",
    "#    z = z.T\n",
    "#    contour(u, v, z)\n",
    "#    title('lambda = %f' % l)\n",
    "#    xlabel('Microchip Test 1')\n",
    "#    ylabel('Microchip Test 2')\n",
    "#    legend(['y = 1', 'y = 0', 'Decision boundary'])\n",
    "#    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Regressão Linear com Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Na primeira metade desta parte, você implementará a regressão linear com regularização para prever a quantidade de água fluindo de uma barragem usando a mudança do nível da água em um reservatório. Na próxima metade, você realizará diagnósticos dos algoritmos de aprendizado por meio de depuração e irá examinar os efeitos de viés (_bias_) e da variância (_variance_).\n",
    "\n",
    "Você irá usar o dataset fornecido no arquivo **ex5data1.mat$^1$**. Nesse arquivo há registros históricos na mudança no nível da água, _x_, e da quantidade de água que sai da barragem, _y_. Este conjunto de dados é dividido em três partes:\n",
    " - Um conjunto de treinamento que seu modelo aprenderá em: _**X**_, _**y**_\n",
    " - Um conjunto de validação cruzada para determinar o parâmetro de regularizacão: _**Xval**_, _**yval**_\n",
    " - Um conjunto de testes para avaliar o desempenho. Estes são exemplos que seu modelo não irá usar durante o treino: _**Xtest**_, _**ytest**_\n",
    "\n",
    "Os nomes das variáveis contidas no arquivo **ex5data1.mat** são os seguintes: _X_, _Xtest_, _Xval_, _y_, _ytest_, _yval_. Você irá precisar desses nomes para carregar os dados do arquivo para usar em seus scripts em Python.\n",
    "\n",
    "_____________________________________\n",
    "$^1$ Arquivos com a extensão **_.mat_** são normalmente criados no **Octave** ou no **Matlab**. Para\n",
    "carregar esse arquivo no Python, você pode usar o procedimento descrito em <http://www.\n",
    "blogforbrains.com/blog/2014/9/6/loading-matlab-mat-data-in-python>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carregando o arquivo gerado pelo MATLAB\n",
    "\n",
    "import scipy.io\n",
    "dfile = scipy.io.loadmat('am-T2-dados/ex5data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Visualização dos Dados\n",
    "\n",
    "Você deve começar por produzir uma visualização do conjunto de dados de treinamento. O gráfico que você deve produzir deve ser similar ao apresentado na Figura abaixo:\n",
    "\n",
    "![scatter_trainning.jpg](scatter_trainning.jpg)\n",
    "\n",
    "\n",
    "### 2.2 Função de Custo de Regressão Linear Regularizada\n",
    "\n",
    "Lembre-se de que a regressão linear regularizada tem a seguinte função de custo:\n",
    "\n",
    "$$J_{regularizado} = \\small {\\frac{1}{2m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(x^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- x^{(2)}\\right) \\large{)} } + {\\frac{\\lambda}{2m} \\sum\\limits_{j = 1}^{n}\\ {\\theta}_{j}^{2} } $$\n",
    "\n",
    "Lembre-se de quê um hyperparâmetro que controla o grau de regularização (e assim, ajuda a prevenir o excesso de sobreajuste). O termo de regularização impõe uma penalidade sobre o custo total J**${\\theta}$**. Conforme as magnitudes dos parâmetros do modelo J aumentam, a penalização aumenta também. Note que você não deve regularizar o termo **${\\theta}$**.\n",
    "\n",
    "Sua tarefa é escrever uma função para calcular a função de custo da regressão linear regularizada. Você deve implementar esse código em um arquivo de nome ${linearRegCostFunction.py}$. Se possível, tente vetorizar seu código e evitar o uso de _loops_. Quando você tiver completado a implementação, verifique a corretude da sua função de custo usando ${\\theta}$ inicializado com (1, 1). Você deve esperar ver uma saída de _303.993_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Gradiente na regressão linear regularizada\n",
    "\n",
    "A derivada parcial do gradiente da função de custo da regressão linear regularizada é um vetor no qual o *j-ésimo* elemento é definido conforme a seguir:\n",
    "\n",
    "\n",
    "No arquivo $linearRegCostFunction.py$, adicione código para calcular o gradiente, Quando você tiver completado essa implementação, teste a corretude usando $theta$ inicializado em (1, 1). Você deve esperar ver um gradiente de (-15.30, 598.250)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4 Ajustando os parâmetros da regressão linear\n",
    "\n",
    "Nesta parte, use a função **linearRegCostFunction** para computar os valores ótimos para $\\theta$, mas sem usar regularização, i.e., defina $\\lambda$ = 0. Após isso, construa um gráfico para visualiar o modelo construído. Seu gráfico deve ser similar ao apresentado na Figura 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3 Viés-Variância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Um conceito importante no Aprendizado de Máquina é o relacionamento entre o **viés (bias) e a variância (variance)** de um modelo de aprendizado. Modelos com viés elevado não são suficientemente complexos para os dados e tendem a sofrer de subajuste (underfitting), enquanto que modelos com alta variância tendem a sofrer de sobreajuste.\n",
    "\n",
    "Nesta parte do trabalho, você irá produzir gráficos dos erros de treinamento e teste na forma de curvas de aprendizado para diagnosticar problemas de **_viés-variância_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.1 Curvas de Aprendizado\n",
    "\n",
    "Agora voc^e implementar\u0013a c\u0013odigo para gerar as curvas de aprendizado que ser~ao \u0013uteis na depura\u0018c~ao de algoritmos de aprendizagem. Lembre-se de que uma curva de aprendizagem tra\u0018ca erros de treinamento e de valida\u0018c~ao cruzada\n",
    "como fun\u0018c~oes do tamanho do conjunto de treinamento. Crie um arquivo\n",
    "de nome learningCurve.py que deve conter uma fun\u0018c~ao (tamb\u0013em chamada\n",
    "learningCurve) que retorna um vetor de erros para o conjunto de treinamento\n",
    "e conjunto de valida\u0018c~ao cruzada.\n",
    "\n",
    "Para tra\u0018car a curva de aprendizado, precisamos de um conjunto de treinamento\n",
    "e valida\u0018c~ao cruzada erro para diferentes tamanhos de conjuntos de\n",
    "treinamento. Para obter diferentes tamanhos de conjuntos de treinamento,\n",
    "voc^e deve usar subconjuntos diferentes do conjunto de treinamento original X.\n",
    "Especi\f",
    "camente, para um tamanho de conjunto de treinamento de i, voc^e deve\n",
    "usar os primeiros exemplos de i (ou seja, X(1 : i; :) e y(1 : i)).\n",
    "\n",
    "Para cada tamanho de conjunto de treinamento, voc^e encontrar os par^ametros\n",
    "\u0012. Note que o lambda deve ser passado como um par^ametro para a fun\u0018c~ao\n",
    "learningCurve. Depois de aprender os par^ametros \u0012, voc^e deve calcular o\n",
    "erro nos conjuntos de treinamento e de valida\u0018c~ao. Lembre-se de que o erro de\n",
    "treinamento para um conjunto de dados \u0013e de\f",
    "nido como:\n",
    "\n",
    "Em particular, note que o erro de treinamento n~ao inclui o termo de regulariza\n",
    "\u0018c~ao. Uma maneira de calcular o erro de treinamento \u0013e usar a sua fun\u0018c~ao\n",
    "de custo j\u0013a implementada e de\f",
    "nir \u0015 = 0 apenas para calcular o erro de treinamento\n",
    "e o erro de valida\u0018c~ao cruzada. Quando voc^e estiver computando o\n",
    "erro no conjunto de treinamento, certi\f",
    "que-se de comput\u0013a-lo no subconjunto\n",
    "de treinamento (ou seja, X(1 : n; :) e y(1 : n)), em vez de usar todo o conjunto\n",
    "de treinamento). No entanto, para o erro de valida\u0018c~ao cruzada, voc^e deve calcul\n",
    "\u0013a-lo usando todo o conjunto de valida\u0018c~ao cruzada. Voc^e deve armazenar os erros calculados em dois vetores.\n",
    "Quando voc^e estiver terminado o que foi descrito acima, imprima as curvas\n",
    "de aprendizado e produza um gr\u0013a\f",
    "co similar ao apresentado na Figura 5.\n",
    "\n",
    "![plot_linear.jpg](plot_linear.jpg)\n",
    "\n",
    "Na curva que voc^e ir\u0013a produzir, voc^e poder\u0013a observar que os erros de treinamento\n",
    "e de valida\u0018c~ao cruzada s~ao ambos altos quando o n\u0013umero de exemplos\n",
    "de treinamento \u0013e aumentado. Isso re\n",
    "ete o vi\u0013es alto do modelo (o modelo de\n",
    "regress~ao linear \u0013e muito simples e n~ao consegue se ajustear bem ao conjunto\n",
    "de dados). Na pr\u0013oxima se\u0018c~ao, voc^e ir\u0013a implementar regress~ao polinomial para\n",
    "ajustar um modelo melhor a este conjunto de dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Regressão Polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "O problema com nosso modelo linear \u0013e que ele \u0013e muito simples para os dados e\n",
    "resultou em subajuste (vi\u0013es alto). Nesta parte, voc^e ir\u0013a resolver esse problema\n",
    "adicionando mais caracter\u0013\u0010sticas. Para usar a regress~ao polinomial, de\f",
    "na uma\n",
    "hip\u0013otese da seguinte forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5 Regressão Polinomial - aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Depois de ter completado $polyFeatures.py$, você deve treinar um modelo de regressão polinomial usando sua função de custo da regressão linear. Tenha em mente que, apesar de termos termos polinomiais no vector de características, ainda estamos resolvendo um problema de otimização de regressão linear. Os termos polinomiais simplesmente se transformaram em características que podemos usar para aplicar regressão linear. Estamos usando a mesma função de custo e gradiente que você implementou para a parte anterior deste trabalho.\n",
    "\n",
    "Para esta parte do trabalho, você usará um polinômio de grau 8. Se executarmos o treinamento diretamente sobre os dados projetados, não iremos obter um bom resultado, porque as características não irão estar na mesma escala (por exemplo, um exemplo com x = 40 agora terá uma característica $xˆ8 = 40ˆ8 = 6.5 x 10ˆ1 2$). \n",
    "\n",
    "Portanto, você vai precisar aplicar a normalização de características. Portanto, antes de aprender os parâmetros $\\theta$ para a regressão polinomial, você deve normalizar as características do conjunto de treinamento, e armazenar os parâmetros $\\ho$ e $\\sigma$. Depois de aprender os parâmetros $\\theta$, você deve ver gerar dois gráficos (que devem ser similares aos das Figuras 6 e 7) gerados com a regressão polinomial com $\\lambda$ = 0.\n",
    "\n",
    "![plot_polinomial.jpg](plot_polinomial.jpg)\n",
    "\n",
    "Da Figura 6, voc^e deve perceber que o polin^omio pode se ajustar aos pontos de dados muito bem - assim, obtendo um baixo erro de treinamento. No entanto, o polin^omio \u0013e muito complexo e at\u0013e mesmo despenca nos extremos. Isso \u0013e um indicador de que o modelo de regress~ao polinomial est\u0013a se ajustando demasiadamente aos dados de treinamento e que n~ao ir\u0013a generalizar bem. \n",
    "\n",
    "![plot_curva_aprendizado.jpg](plot_curva_aprendizado.jpg)\n",
    "\n",
    "Para entender melhor os problemas com o modelo n~ao regularizado (\u0015 = 0), voc^e pode ver que a curva de aprendizado (Figura 7) apresenta erro de treinamento baixo, mas erro de valida\u0018c~ao alto. H\u0013a uma lacuna entre os erros de treinamento e valida\u0018c~ao cruzada, indicando um problema de vari^ancia alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Tarefas adicionais (OPCIONAIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Ajuste do par^ametro de regulariza\u0018c~ao. Teste o aprendizado para diferentes valores de lambda, 1 \u0014 \u0015 \u0014 100. Para \u0015 = 1, voc^e deve ver o polin^omio segue a tend^encia de dados bem (Figura 6) e uma curva de aprendizado (Figura 7) mostrando que tanto o erro de valida\u0018c~ao quanto o erro de treinamento convergem para um valor relativamente baixo. Isso mostra que o modelo de regress~ao polinomial regularizado com \u0015 = 1 n~ao sofre dos problema de vi\u0013es alto ou de vari^ancia alta. De fato, esse modelo alcan\u0018ca um bom compromisso entre vi\u0013es e vari^ancia. Para \u0015 = 100, voc^e deve ver um polin^omio (Figura 8) que n~ao segue os dados bem. Neste caso, h\u0013a muita regulariza\u0018c~ao, e o modelo \u0013e incapaz de se ajustar aos dados de treinamento.\n",
    "\n",
    " - De\f",
    "na o valor de \u0015 usando o conjunto de valida\u0018c~ao. Implemente um m\u0013etodo automatizado para selecionar o par^ametro \u0015. Concretamente, use um conjunto de valida\u0018c~ao cruzada para avaliar qu~ao bom \u0013e cada valor de \u0015. Tente valores de \u0015 no intervalo f0; 0:001; 0:003; 0:01; 0:03; 0:1; 0:3; 1; 3; 10g. Ao realizar essa tarefa, voc^e deve encontrar um valor adequado de \u0015 en torno de 3 (veja a Figura 8).\n",
    "\n",
    " - Depois de selecionar o melhor valor de \u0015 usando o conjunto de valida\u0018c~ao, voc^e pode ent~ao avaliar o modelo no conjunto de teste para estimar o qu~ao bem ele ser\u0013a executado em dados reais n~ao vistos.\n",
    " \n",
    "![plot_lambda.jpg](plot_lambda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7 O que deve ser entregue\n",
    "\n",
    "Voc^e deve preparar um \u0013unico relat\u0013orio para a apresentar sua an\u0013alise e conclus~oes\n",
    "sobre as diversas partes desse trabalho. O formato desse relat\u0013orio deve ser em\n",
    "PDF. Alternativamente \u0012a entrega do relat\u0013orio em PDF, voc^e pode entregar um\n",
    "notebook Jupyter2.\n",
    "\n",
    "Independente de escolher entregar um relat\u0013orio em PDF ou na forma de\n",
    "um notebook Jupyter, entregue tamb\u0013em todos os arquivos em Python que voc^e\n",
    "criou para cada parte deste trabalho. Todos os arquivos em Python deve estar\n",
    "em uma \u0013unica pasta.\n",
    "\n",
    "Crie um arquivo compactado que cont\u0013em o relat\u0013orio (ou notebook Jupyter)\n",
    "e os arquivos (scripts) em Python. Esse arquivo compactado deve se chamar\n",
    "SEU NOME COMPLETO T1.zip. Esse arquivo compactado deve ser entregue pelo\n",
    "Moodle, até a data acordada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APENDICE\n",
    "\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n",
    "\n",
    "**Exercise**: Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
